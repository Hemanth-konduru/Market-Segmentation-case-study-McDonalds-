# -*- coding: utf-8 -*-
"""Market Segmentation case study.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CZrP5Uu7MOVTKol7yghOdp5woBE4iDDI

##Case Study: Fast Food McDonalds

###Deciding (not) to Segment
McDonald's can choose to take a wide strategy, meaning that it will service the whole market without having to go into detail about the distinctions between different client segments. Alternatively, even with its considerable market power, McDonald's might see the advantages of knowing customers' varied tastes and use that information to launch a focused marketing campaign.

###Specifying the idea Target Segment
The management of McDonald's must determine which salient characteristics draw them into a certain market group. Regarding the knock-out criteria, the target segment or segments must be large enough to support the development of the criteria, distinct i.e., members of the segments differ significantly from members of other segments in a key characteristic, homogeneous i.e., segment members are similar to one another in a key characteristic.

###Data Collecting
1453 adult Australian consumers were asked to rate McDonald's on a number of qualities, including YUMMY, CONVENIENT, SPICY, FATTENING, GREASY, FAST, CHEAP, TASTY, EXPENSIVE, HEALTHY, and DISGUSTING. The responses are included in the dataset. Prior to the survey, a qualitative investigation was carried out in order to identify these features. Every participant stated if they thought McDonald's had each quality Yes or no.
In addition, the respondents' Age and Gender were disclosed. More information about eating out habits and favorite information sources would probably be acquired if this data were being gathered for a real market segmentation study. Having this extra data would make it easier to describe each market group in greater detail and in greater detail.


###Exploring Data
Initially, we examine the fundamental characteristics of the data set by loading it and examining elements such variable names and the initial rows of the data. Along with it we check for any anomalies in data such as null values invalid values missing values etc.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from warnings import filterwarnings
filterwarnings("ignore")

data_set_link = "https://homepage.boku.ac.at/leisch/MSA/datasets/mcdonalds.csv"

data = pd.read_csv(data_set_link)

data.head()

data.info()

data.columns

data.shape

data.isnull().sum()

data['Gender'].value_counts()

data['Like'].value_counts()

data['VisitFrequency'].value_counts()

columns = data.iloc[:,0:11].columns
print(columns)

plt.figure(figsize=(15,25))
x=1
for col in columns:
  plt.subplot(7,2,x)
  sns.countplot(x=col, data=data, palette="Set2")
  x+=1

sns.countplot(x="Gender", data=data, palette="Set1_r")

size = data['Gender'].value_counts()
labels = ["Female","Male"]
color = ['green', 'blue']
explode=[0, 0.1]
plt.pie(size, colors = color, explode = explode, labels = labels, shadow = True, autopct = '%.2f%%')
plt.title('Gender', fontsize = 20)
plt.axis('off')
plt.legend()
plt.show()

plt.rcParams['figure.figsize'] = (25, 8)
df = sns.countplot(x=data['Age'], palette = 'viridis')
df.bar_label(df.containers[0])
plt.title('Age distribution of customers', fontsize = 20)
plt.show()

data['Like'].value_counts()

data['Like'] = data['Like'].replace({"I hate it!-5":"-5", "I love it!+5": "+5"})

sns.catplot(data=data, x="Like", y="Age", orient="v", height=5, aspect=2, palette="Set2",kind="swarm")
plt.title('Likelyness of McDonald with respect to Age', fontsize=20)
plt.show()

"""###Data preprocessing
Creating a new DataFrame for the required columns
"""

new_data = data.drop(['Like','Gender','VisitFrequency','Age'], axis=1)
new_data.head()

!pip install -U bioinfokit
!pip install -U yellowbrick
!pip install -U statsmodels

from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn import preprocessing
from bioinfokit.visuz import cluster
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
from collections import Counter
from statsmodels.graphics.mosaicplot import mosaic
from itertools import product

data_encode = new_data.apply(LabelEncoder().fit_transform)
data_encode.head()

"""###Applying PCA"""

names = []
for num in range(1,12):
  nm = "pc" + str(num)
  names.append(nm)
  nm = ""
names

pca_data = preprocessing.scale(data_encode)
pca = PCA(n_components=11)
pc = pca.fit_transform(pca_data)

pf = pd.DataFrame(data=pc,columns=names)
pf.head()

pca.explained_variance_ratio_

loadings = pca.components_
num_pc = pca.n_features_
pc_list = ["PC"+str(i) for i in list(range(1, num_pc+1))]
loadings_df = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))
loadings_df['feature'] = data_encode.columns.values
loadings_df = loadings_df.set_index('feature')
loadings_df

plt.rcParams['figure.figsize'] = (15,10)
ax = sns.heatmap(loadings_df, annot=True, cmap='Greens')
plt.show()

pca_scores = PCA().fit_transform(pca_data)
filterwarnings("ignore")
cluster.biplot(cscore=pca_scores, loadings=loadings, labels=data.columns.values,
               var1=round(pca.explained_variance_ratio_[0]*100, 2),
    var2=round(pca.explained_variance_ratio_[1]*100, 2),show=True,dim=(10,5))

"""##Extracting Segments

###Applying The Elbow Method
"""

model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,12)).fit(data_encode)
visualizer.show();

"""###Applying K-means Clustring Algorithm"""

kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0).fit(data_encode)
data['cluster_num'] = kmeans.labels_
print ('Labels:', kmeans.labels_)
print ('WCSS:', kmeans.inertia_)
print('No. of iterations: ', kmeans.n_iter_)
print('Cluster centroids: ', kmeans.cluster_centers_)
print('Cluster size: ', Counter(kmeans.labels_))

sns.scatterplot(data=pf, x="pc1", y="pc2", hue=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], marker="X", c="r", s=80, label="centroids")
plt.legend()
plt.show()

"""###Describing Segments"""

vals = data['Like'].unique()
vals.sort()
vals

crosstab = pd.crosstab(data['cluster_num'], data['Like'])
crosstab = crosstab[vals]
crosstab

plt.rcParams['figure.figsize'] = (7,7)
mosaic(crosstab.stack())
display(crosstab, plt.show())

crosstab_gender = pd.crosstab(data['cluster_num'],data['Gender'])
crosstab_gender

plt.rcParams['figure.figsize'] = (7,5)
mosaic(crosstab_gender.stack())
display(crosstab, plt.show())

sns.boxplot(x="cluster_num", y="Age", data=data)

"""###Select the Target Segment"""

data['VisitFrequency'] = LabelEncoder().fit_transform(data['VisitFrequency'])
visit = data.groupby('cluster_num')['VisitFrequency'].mean()
visit = visit.to_frame().reset_index()
visit

data['Like'] = LabelEncoder().fit_transform(data['Like'])
Like = data.groupby('cluster_num')['Like'].mean()
Like = Like.to_frame().reset_index()
Like

data['Gender'] = LabelEncoder().fit_transform(data['Gender'])
Gender = data.groupby('cluster_num')['Gender'].mean()
Gender = Gender.to_frame().reset_index()
Gender

segment = Gender.merge(Like, on='cluster_num', how='left').merge(visit, on='cluster_num', how='left')
segment

plt.figure(figsize = (9,4))
sns.scatterplot(x = "VisitFrequency", y = "Like",data=segment,s=400, color="r")
plt.title("Simple segment evaluation plot for the fast food data set", fontsize = 15)
plt.xlabel("Visit", fontsize = 12)
plt.ylabel("Like", fontsize = 12)
plt.show()

